# çªå‘å…¬å…±äº‹ä»¶ä¸çŠ¯ç½ªç ”ç©¶å¤šæºçˆ¬è™«

è¯¥é¡¹ç›®èšåˆæ–°é—»ã€ç©ºé—´ã€æˆ¿äº§ä¸æ³•å¾‹æ–‡ä¹¦ç­‰å¤šæºæ•°æ®ï¼Œæ”¯æ’‘çªå‘å…¬å…±äº‹ä»¶é£é™©è¯„ä¼°ä¸çŠ¯ç½ªç ”ç©¶ã€‚æ ¸å¿ƒäº®ç‚¹ï¼š

- ğŸ§© æ¨¡å—åŒ–æ ¸å¿ƒï¼š`core/` ä¸‹çš„å››ç±»çˆ¬è™«ç»§æ‰¿ç»Ÿä¸€åŸºç±»ï¼Œæ”¯æŒåˆ†å¸ƒå¼è°ƒåº¦ä¸æ–­ç‚¹ç»­è·‘ã€‚
- ğŸŒ ç©ºé—´æ™ºèƒ½ï¼šå†…ç½® 5kmÃ—5km ç½‘æ ¼åŒ–ä¸ WGS84 åæ ‡è½¬æ¢ï¼Œè¾“å‡º GeoJSON/PostgreSQLã€‚
- ğŸ”„ æ•°æ®ç®¡çº¿ï¼š`data_pipeline.py` è´Ÿè´£å»é‡ã€æ ¡éªŒã€è§„èŒƒåŒ–å¹¶å†™å…¥æ•°æ®åº“ã€‚
- ğŸ›¡ï¸ åçˆ¬ç­–ç•¥ï¼šç”¨æˆ·ä»£ç†è½®æ¢ã€æŒ‡æ•°é€€é¿ã€ä»£ç†æ± ä¸é¢‘ç‡æ§åˆ¶ã€‚

## å¿«é€Ÿå¼€å§‹

```bash
python -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

é…ç½® `config/crawler_config.yml`ï¼ˆç¤ºä¾‹è§ `config.py` æ³¨é‡Šæˆ–åˆ›å»ºè‡ªå®šä¹‰ YAMLï¼‰ã€‚å¯é€šè¿‡ç¯å¢ƒå˜é‡ `BAIDU_LBS_AK`ã€`POSTGRES_DSN` è¦†ç›–æ•æ„Ÿä¿¡æ¯ã€‚

## è¿è¡Œç¤ºä¾‹

```bash
# å•ä¸ªä»»åŠ¡ï¼ˆé»˜è®¤å†™å…¥ news_recordsï¼‰
python -m crawler_project.main news --start-date 2015-01-01 --end-date 2015-12-31 --table news_events

# å¤šä»»åŠ¡å¹¶å‘è°ƒåº¦ï¼ˆæœ€å¤š 3 ä¸ªå¹¶å‘ï¼‰
python -m crawler_project.main news spatial housing --parallelism 3 --postgres-dsn postgresql+psycopg2://user:pwd@host:5432/db

# Scrapy å…¥å£
scrapy crawl north_news -s LOG_LEVEL=INFO
scrapy crawl housing_market -a max_pages=5
```

> Scrapy ä½äº `crawler_project/scrapy_app`ï¼Œå…¶å…¥å£æ–‡ä»¶ `scrapy.cfg` å·²é…ç½®å®Œæˆï¼Œå¯åœ¨ `crawler_project/` ç›®å½•ä¸‹ç›´æ¥è¿è¡Œ `scrapy crawl <spider_name>`ã€‚

### PostgreSQL è¿æ¥

- é»˜è®¤ DSN ä¸º `config.py` / `config/crawler_config.yml` ä¸­çš„ `storage.postgres_dsn`ã€‚
- è¿è¡Œ CLI æ—¶å¯ä½¿ç”¨ `--postgres-dsn` ä¸´æ—¶è¦†ç›–ã€‚
- è¿æ¥æ± å‚æ•°ï¼ˆ`pool_size`ã€`max_overflow`ã€`pool_timeout`ï¼‰ä¹Ÿå¯åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒæ•´ã€‚

## è‡ªåŠ¨åŒ–æµ‹è¯•

```bash
pytest
```

## ç›®å½•ç»“æ„

```
crawler_project/
â”œâ”€â”€ core/                # å„ç±»çˆ¬è™«å®ç°
â”œâ”€â”€ utils/               # é…ç½®ã€ä»£ç†ã€è§£æã€å­˜å‚¨å·¥å…·
â”œâ”€â”€ scrapy_app/          # Scrapy é¡¹ç›®ï¼ˆspidersã€pipelinesã€settingsï¼‰
â”œâ”€â”€ data_pipeline.py     # æ•°æ®æ ‡å‡†åŒ–ä¸å†™å…¥
â”œâ”€â”€ main.py              # å‘½ä»¤è¡Œå…¥å£
â”œâ”€â”€ logs/                # è¿è¡Œæ—¥å¿—
â”œâ”€â”€ output/              # è¾“å‡ºæ–‡ä»¶
â””â”€â”€ requirements.txt
```

## æ³¨æ„äº‹é¡¹

- ä¸¥æ ¼éµå®ˆç›®æ ‡ç½‘ç«™ robots.txt ä¸ API é€Ÿç‡é™åˆ¶ã€‚
- æ–°é—»/æ³•å¾‹çˆ¬è™«é»˜è®¤å»¶è¿Ÿ â‰¥2sï¼›API ä»»åŠ¡æ ¹æ®å¹³å°é™æµè°ƒåº¦ã€‚
- æ•æ„Ÿå­—æ®µåœ¨å…¥åº“å‰è°ƒç”¨è„±æ•é€»è¾‘ï¼ˆå¯åœ¨ `data_pipeline` ä¸­æ‰©å±•ï¼‰ã€‚
- ä½¿ç”¨ä»£ç†æ± å‰ï¼Œåœ¨ `utils/proxy_manager.py` é…ç½®åœ°å€å¹¶å¯ç”¨ `config.ProjectSettings.proxy`ã€‚
